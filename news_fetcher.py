"""
News Fetcher Module
è·å–é»„é‡‘ç›¸å…³çš„å®è§‚æ–°é—»ï¼ˆRSS feeds + NewsAPIï¼‰
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import json
import os


class NewsFetcher:
    """æ–°é—»è·å–å™¨"""
    
    # å…è´¹çš„ RSS æºï¼ˆèšç„¦äºå®è§‚ç»æµã€å¤®è¡Œã€åœ°ç¼˜æ”¿æ²»ï¼‰
    RSS_FEEDS = [
        # å¤®è¡Œä¸è´§å¸æ”¿ç­–
        "https://www.federalreserve.gov/feeds/press_all.xml",
        "https://www.ecb.europa.eu/rss/press.html",
        
        # é‡‘èæ–°é—»
        "https://www.reuters.com/finance/markets/feed/",
        "https://feeds.bloomberg.com/markets/news.rss",
        
        # é»„é‡‘ä¸“é—¨
        "https://www.kitco.com/rss/KitcoNews.xml",
        "https://www.mining.com/rss/",
        
        # å®è§‚ç»æµ
        "https://www.imf.org/en/News/RSS",
    ]
    
    # å…³é”®è¯è¿‡æ»¤ï¼ˆåªä¿ç•™ä¸é»„é‡‘ç›¸å…³çš„æ–°é—»ï¼‰
    GOLD_KEYWORDS = [
        'gold', 'xau', 'precious metal', 'bullion',
        'federal reserve', 'fed', 'interest rate', 'inflation',
        'dollar', 'dxy', 'treasury', 'yield',
        'geopolitical', 'war', 'sanction', 'crisis',
        'central bank', 'monetary policy', 'ecb', 'boj'
    ]
    
    def __init__(self, news_api_key: Optional[str] = None):
        self.news_api_key = news_api_key
        self.cache_dir = "news_cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def fetch_from_rss(self, max_age_hours: int = 48) -> List[Dict]:
        """
        ä» RSS feeds è·å–æ–°é—»
        
        Args:
            max_age_hours: åªè·å–æœ€è¿‘Nå°æ—¶çš„æ–°é—»
            
        Returns:
            List of news items
        """
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=max_age_hours)
        
        print(f"\nğŸ“° Fetching news from {len(self.RSS_FEEDS)} RSS feeds...")
        
        for feed_url in self.RSS_FEEDS:
            try:
                feed = feedparser.parse(feed_url)
                source = feed.feed.get('title', feed_url)
                
                for entry in feed.entries[:20]:  # æ¯ä¸ªæºæœ€å¤šå–20æ¡
                    # è§£ææ—¶é—´
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    
                    # æ£€æŸ¥æ—¶é—´èŒƒå›´
                    if pub_date and pub_date < cutoff_time:
                        continue
                    
                    # æå–å†…å®¹
                    title = entry.get('title', '')
                    summary = entry.get('summary', entry.get('description', ''))
                    link = entry.get('link', '')
                    
                    # å…³é”®è¯è¿‡æ»¤
                    content = (title + ' ' + summary).lower()
                    if not any(kw in content for kw in self.GOLD_KEYWORDS):
                        continue
                    
                    all_news.append({
                        'source': source,
                        'title': title,
                        'summary': summary,
                        'url': link,
                        'published': pub_date.isoformat() if pub_date else None,
                        'fetched_at': datetime.now().isoformat()
                    })
                
                print(f"  âœ“ {source}: {len([n for n in all_news if n['source'] == source])} articles")
                
            except Exception as e:
                print(f"  âœ— Error fetching {feed_url}: {e}")
        
        print(f"\nâœ“ Total fetched: {len(all_news)} relevant articles")
        return all_news
    
    def fetch_from_newsapi(self, query: str = "gold OR inflation OR federal reserve", 
                           days: int = 2) -> List[Dict]:
        """
        ä» NewsAPI è·å–æ–°é—»ï¼ˆéœ€è¦ API keyï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            days: å›æº¯å¤©æ•°
            
        Returns:
            List of news items
        """
        if not self.news_api_key:
            print("âš  NewsAPI key not provided, skipping NewsAPI")
            return []
        
        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': query,
                'from': (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'),
                'language': 'en',
                'sortBy': 'publishedAt',
                'apiKey': self.news_api_key,
                'pageSize': 50
            }
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            articles = data.get('articles', [])
            
            news_items = []
            for article in articles:
                news_items.append({
                    'source': article.get('source', {}).get('name', 'Unknown'),
                    'title': article.get('title', ''),
                    'summary': article.get('description', ''),
                    'url': article.get('url', ''),
                    'published': article.get('publishedAt', None),
                    'fetched_at': datetime.now().isoformat()
                })
            
            print(f"âœ“ NewsAPI: {len(news_items)} articles")
            return news_items
            
        except Exception as e:
            print(f"âœ— Error fetching from NewsAPI: {e}")
            return []
    
    def fetch_all_news(self, max_age_hours: int = 48) -> List[Dict]:
        """
        è·å–æ‰€æœ‰æ–°é—»æº
        
        Args:
            max_age_hours: æœ€è¿‘Nå°æ—¶
            
        Returns:
            Combined list of news items
        """
        all_news = []
        
        # RSS feeds
        rss_news = self.fetch_from_rss(max_age_hours)
        all_news.extend(rss_news)
        
        # NewsAPI (å¦‚æœæœ‰ key)
        if self.news_api_key:
            api_news = self.fetch_from_newsapi(days=max_age_hours//24 + 1)
            all_news.extend(api_news)
        
        # å»é‡ï¼ˆåŸºäº URLï¼‰
        seen_urls = set()
        unique_news = []
        for item in all_news:
            url = item.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_news.append(item)
        
        # æŒ‰æ—¶é—´æ’åº
        unique_news.sort(
            key=lambda x: x.get('published', ''), 
            reverse=True
        )
        
        # ä¿å­˜åˆ°ç¼“å­˜
        cache_file = os.path.join(
            self.cache_dir, 
            f"news_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        )
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(unique_news, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ Total unique articles: {len(unique_news)}")
        print(f"âœ“ Cached to {cache_file}")
        
        return unique_news
    
    def get_calendar_events(self) -> List[Dict]:
        """
        è·å–ç»æµæ—¥å†äº‹ä»¶ï¼ˆMVP: ç¡¬ç¼–ç å¸¸è§äº‹ä»¶ï¼‰
        åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥æ¥å…¥ Trading Economics API ç­‰
        
        Returns:
            List of calendar events
        """
        # MVP: è¿”å›å¸¸è§çš„å‘¨æœŸæ€§äº‹ä»¶
        # çœŸå®å®ç°éœ€è¦ä» API è·å–
        events = [
            {
                'event_type': 'MACRO_DATA_RELEASE',
                'name': 'US CPI',
                'schedule': 'Monthly, ~13th',
                'impact': 'High',
                'next_date': None  # éœ€è¦å®æ—¶ API
            },
            {
                'event_type': 'MACRO_DATA_RELEASE',
                'name': 'US PCE',
                'schedule': 'Monthly, end of month',
                'impact': 'High',
                'next_date': None
            },
            {
                'event_type': 'CENTRAL_BANK_DECISION',
                'name': 'FOMC Meeting',
                'schedule': '~8 times per year',
                'impact': 'Very High',
                'next_date': None
            },
            {
                'event_type': 'MACRO_DATA_RELEASE',
                'name': 'US Jobs Report',
                'schedule': 'First Friday of month',
                'impact': 'High',
                'next_date': None
            }
        ]
        
        return events


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è¯»å– API keyï¼ˆå¯é€‰ï¼‰
    news_api_key = os.getenv('NEWS_API_KEY')
    
    fetcher = NewsFetcher(news_api_key=news_api_key)
    
    # è·å–æ–°é—»
    news = fetcher.fetch_all_news(max_age_hours=72)
    
    if news:
        print("\nğŸ“„ Sample news items:")
        for item in news[:5]:
            print(f"\n- {item['title']}")
            print(f"  Source: {item['source']}")
            print(f"  Published: {item['published']}")
    
    # è·å–æ—¥å†äº‹ä»¶
    calendar = fetcher.get_calendar_events()
    print(f"\nğŸ“… Calendar events: {len(calendar)} items")
